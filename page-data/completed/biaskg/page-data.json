{"componentChunkName":"component---node-modules-gatsby-theme-garden-src-templates-local-file-js","path":"/completed/biaskg","result":{"data":{"file":{"childMdx":{"body":"var _excluded = [\"components\"];\nfunction _extends() { return _extends = Object.assign ? Object.assign.bind() : function (n) { for (var e = 1; e < arguments.length; e++) { var t = arguments[e]; for (var r in t) ({}).hasOwnProperty.call(t, r) && (n[r] = t[r]); } return n; }, _extends.apply(null, arguments); }\nfunction _objectWithoutProperties(e, t) { if (null == e) return {}; var o, r, i = _objectWithoutPropertiesLoose(e, t); if (Object.getOwnPropertySymbols) { var n = Object.getOwnPropertySymbols(e); for (r = 0; r < n.length; r++) o = n[r], -1 === t.indexOf(o) && {}.propertyIsEnumerable.call(e, o) && (i[o] = e[o]); } return i; }\nfunction _objectWithoutPropertiesLoose(r, e) { if (null == r) return {}; var t = {}; for (var n in r) if ({}.hasOwnProperty.call(r, n)) { if (-1 !== e.indexOf(n)) continue; t[n] = r[n]; } return t; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"biaskg-adversarial-knowledge-graphs-to-induce-bias-in-large-language-models\"\n  }, \"BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/VectorInstitute/biaskg\"\n  }, \"Github\")), mdx(\"p\", null, \"Modern large language models (LLMs) have a significant amount of world knowledge, which enables strong performance in commonsense reasoning and knowledge-intensive tasks when harnessed properly. The language model can also learn social biases, which has a significant potential for societal harm. There have been many mitigation strategies proposed for LLM safety, but it is unclear how effective they are for eliminating social biases. \"), mdx(\"p\", null, \"In this work, we propose a new methodology for attacking language models with knowledge graph-augmented generation. We refactor natural language stereotypes into a knowledge graph, and use adversarial attacking strategies to induce biased responses from several open- and closed-source language models. We find our method increases bias in all models, even those trained with safety guardrails. This demonstrates the need for further research in AI safety, and further work in this new adversarial space.\"), mdx(\"h2\", {\n    \"id\": \"significance\"\n  }, \"Significance\"), mdx(\"p\", null, \"This project studied:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"How to create a knowledge graph from natural language phrases\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"How to attack language models in a thorough, automated way\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"How social bias can change depending on the input prompt\")), mdx(\"h2\", {\n    \"id\": \"post-mortem\"\n  }, \"Post mortem\"), mdx(\"p\", null, \"I like the knowledge graph portion of this project, but I think the application to red-teaming is somewhat impractical now that the field is more mature. It was a good first attempt in a relatively new space, but red-teaming LLMs seems to be a fool's errand. There will always be a corner that is untested, and unlike traditional software, the cost of patching a hole is more than modifying some code - you would need to retrain the model, which could be extremely expensive even for a few training steps.\"), mdx(\"p\", null, \"In general, this project made me realize poorly trained LLMs still have all of the flaws of their predecessors. Minor changes in the input can result in significant changes to the output. Also, the temperature and other sampling parameters could change the result significantly. Recent models have removed temperature completely, probably for this reason. I realized that publishing on API-based models was also a fool's errand. We performed experiments with the same model (GPT-3.5) several months apart and received completely different results.\"), mdx(\"p\", null, \"Recent works in red-teaming look at the hidden representations instead of the final outputs, and I think these draw deeper insights into refusals. I might revisit this project from that lens, but I also have other projects that I would revisit first.\"));\n}\n;\nMDXContent.isMDXComponent = true;","outboundReferences":[],"inboundReferences":[{"__typename":"Mdx","body":"var _excluded = [\"components\"];\nfunction _extends() { return _extends = Object.assign ? Object.assign.bind() : function (n) { for (var e = 1; e < arguments.length; e++) { var t = arguments[e]; for (var r in t) ({}).hasOwnProperty.call(t, r) && (n[r] = t[r]); } return n; }, _extends.apply(null, arguments); }\nfunction _objectWithoutProperties(e, t) { if (null == e) return {}; var o, r, i = _objectWithoutPropertiesLoose(e, t); if (Object.getOwnPropertySymbols) { var n = Object.getOwnPropertySymbols(e); for (r = 0; r < n.length; r++) o = n[r], -1 === t.indexOf(o) && {}.propertyIsEnumerable.call(e, o) && (i[o] = e[o]); } return i; }\nfunction _objectWithoutPropertiesLoose(r, e) { if (null == r) return {}; var t = {}; for (var n in r) if ({}.hasOwnProperty.call(r, n)) { if (-1 !== e.indexOf(n)) continue; t[n] = r[n]; } return t; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Completed Projects\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"completed-projects\"\n  }, \"Completed Projects\"), mdx(\"p\", null, \"Here you can find the repositories and papers for projects I would consider complete-ish. Some pages have post-mortem reflections - what I could have done better, what is left to be explored, etc.\"), mdx(\"p\", null, \"These are all projects where I was the first author; this means I wrote the entire codebase (except for BiasKG, where I designed the core knowledge graph algorithm and my co-author Ahmed implemented it, and then we ran half of the experiments each) and paper.\"), mdx(\"h2\", {\n    \"id\": \"interpretability\"\n  }, \"Interpretability\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/prototypes\",\n    \"title\": \"prototypes\"\n  }, \"[[prototypes]]\"), \" - Making citation prediction more easily understood by lawyers with prototype theory, precedents, and legislation\"), mdx(\"h2\", {\n    \"id\": \"datasets\"\n  }, \"Datasets\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/legalhatespeech\",\n    \"title\": \"legalhatespeech\"\n  }, \"[[legalhatespeech]]\"), \" - Grounding the subjectivity of hate speech in 11 definitions from various legal authorities\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/misinformation\",\n    \"title\": \"misinformation\"\n  }, \"[[misinformation]]\"), \" - Grounding the assessment of misinformation in possible legal implications\"), mdx(\"h2\", {\n    \"id\": \"bias\"\n  }, \"Bias\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/biaskg\",\n    \"title\": \"biaskg\"\n  }, \"[[biaskg]]\"), \" - Building a dynamic knowledge graph from natural language for assessing social bias in LLMs\"), mdx(\"h2\", {\n    \"id\": \"test-time-alignment\"\n  }, \"Test-time Alignment\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/sae-pd\",\n    \"title\": \"sae-pd\"\n  }, \"[[sae-pd]]\"), \" - Investigating model steering with Sparse Autoencoders (SAEs) for pluralistic alignment on sparse legal data\"));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"__typename":"File","id":"43c4e06e-a3ed-5032-881b-07cea9e40167","fields":{"slug":"/indexes/completed-projects","title":"Completed Projects"}}}]},"fields":{"slug":"/completed/biaskg","title":"BiasKG: Adversarial Knowledge Graphs to Induce Bias in Large Language Models"}}},"pageContext":{"id":"92072389-ab37-5d80-968d-f3417aa39fd8"}},"staticQueryHashes":["2098632890","2221750479","2468095761"]}