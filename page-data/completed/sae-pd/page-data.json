{"componentChunkName":"component---node-modules-gatsby-theme-garden-src-templates-local-file-js","path":"/completed/sae-pd","result":{"data":{"file":{"childMdx":{"body":"var _excluded = [\"components\"];\nfunction _extends() { return _extends = Object.assign ? Object.assign.bind() : function (n) { for (var e = 1; e < arguments.length; e++) { var t = arguments[e]; for (var r in t) ({}).hasOwnProperty.call(t, r) && (n[r] = t[r]); } return n; }, _extends.apply(null, arguments); }\nfunction _objectWithoutProperties(e, t) { if (null == e) return {}; var o, r, i = _objectWithoutPropertiesLoose(e, t); if (Object.getOwnPropertySymbols) { var n = Object.getOwnPropertySymbols(e); for (r = 0; r < n.length; r++) o = n[r], -1 === t.indexOf(o) && {}.propertyIsEnumerable.call(e, o) && (i[o] = e[o]); } return i; }\nfunction _objectWithoutPropertiesLoose(r, e) { if (null == r) return {}; var t = {}; for (var n in r) if ({}.hasOwnProperty.call(r, n)) { if (-1 !== e.indexOf(n)) continue; t[n] = r[n]; } return t; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"towards-low-resource-alignment-to-diverse-perspectives-with-sparse-feedback\"\n  }, \"Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/chufeiluo/SAE-PD\"\n  }, \"Github\")), mdx(\"p\", null, \"As language models have a greater impact on society, it is important to ensure they are aligned to a diverse range of perspectives and are able to reflect nuance in human values. However, the most popular training paradigms for modern language models often assume there is one optimal answer for every query, leading to generic responses and poor alignment. \"), mdx(\"p\", null, \"In this work, we aim to enhance pluralistic alignment of language models in a low-resource setting with two methods: pluralistic decoding and model steering. We empirically demonstrate that model steering offers consistent improvement over zero-shot and few-shot baselines with only 50 annotated samples. Our proposed methods decrease false positives in several high-stakes tasks such as hate speech detection and misinformation detection, and improves the distributional alignment to human values from different demographics. \"), mdx(\"p\", null, \"We hope our work highlights the importance of diversity and how language models can be adapted to consider nuanced perspectives.\"), mdx(\"h2\", {\n    \"id\": \"significance\"\n  }, \"Significance\"), mdx(\"p\", null, \"This project studies:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"How to do model alignment without model training\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Model steering on some abstract attribute (\\\"feedback\\\") instead of something concrete (e.g. hallucinations)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"How to combine multiple steering vectors at the decoding stage (works with or without model steering)\")), mdx(\"h2\", {\n    \"id\": \"post-mortem\"\n  }, \"Post-mortem?\"), mdx(\"p\", null, \"To be honest, I'm not really satisfied with this project to the point that I hesitate to call this a post-mortem. I'm not leaving this project here, to be sure. I think the idea itself could be refined a little more. At its core I'm trying to ask: how can I use expert feedback efficiently? Can I emulate the behaviour of a model that has seen expert feedback, without needing an expert to give feedback on every single sample? I'm not sure how much that came across\"), mdx(\"p\", null, \"I read a paper recently (\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://arxiv.org/abs/2406.02543\"\n  }, \"To believe or not to believe your LLM\"), \") and it gave me new ideas for this paper. Basically, for any predicted token, you can measure the LLM's certainty in its prediction based on the change in probability over repetitions in the input context. If the probability fluctuates/drops when there are alternate answers in the context, this implies model is more easily influenced into changing its answer, so it's less sure. A very intuitive idea, and it works empirically.\"), mdx(\"p\", null, \"In the context of my pluralistic decoding strategy, Pluralistic decoding, we perform 3 steps:\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Take the difference in logit distribution between the base + steered logit\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Measuring the entropy of the difference\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Weighing the difference by the magnitude of entropy, add the displacement back to the base logit\")), mdx(\"p\", null, \"Intuitively, the more the steered logit changed compared to the base, the more it would influence the final prediction logit. I was thinking this would reduce redundant information (i.e. we care about the logits that change significantly compared to the base, and we don't want them to get lost in an average of 6 other logits) but it also, in hindsight, \\\"flattens\\\" the prediction distribution when language models are more easily influenced. It's a kind of calibration in a way - if the model has a 0.9 chance of predicting \\\"Yes\\\" in the base vector, but a 0.9 chance of predicting \\\"No\\\" in one steering direction and a 0.9 chance of predicting \\\"Unclear\\\" in another steering direction, that should flatten out the distribution over all 3 choices. This reduces overconfidence, or places where a model was maybe too certain in the base instance. Maybe there is something there in reducing or detecting hallucinations, but for cheaper than the original paper?\"), mdx(\"p\", null, \"This paper had some interesting results, but I think it's a bit shallow.. I want to revisit this project if I get the time\"));\n}\n;\nMDXContent.isMDXComponent = true;","outboundReferences":[],"inboundReferences":[{"__typename":"Mdx","body":"var _excluded = [\"components\"];\nfunction _extends() { return _extends = Object.assign ? Object.assign.bind() : function (n) { for (var e = 1; e < arguments.length; e++) { var t = arguments[e]; for (var r in t) ({}).hasOwnProperty.call(t, r) && (n[r] = t[r]); } return n; }, _extends.apply(null, arguments); }\nfunction _objectWithoutProperties(e, t) { if (null == e) return {}; var o, r, i = _objectWithoutPropertiesLoose(e, t); if (Object.getOwnPropertySymbols) { var n = Object.getOwnPropertySymbols(e); for (r = 0; r < n.length; r++) o = n[r], -1 === t.indexOf(o) && {}.propertyIsEnumerable.call(e, o) && (i[o] = e[o]); } return i; }\nfunction _objectWithoutPropertiesLoose(r, e) { if (null == r) return {}; var t = {}; for (var n in r) if ({}.hasOwnProperty.call(r, n)) { if (-1 !== e.indexOf(n)) continue; t[n] = r[n]; } return t; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Towards legally enforceable hate speech detection for public forums\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"towards-legally-enforceable-hate-speech-detection-for-public-forums\"\n  }, \"Towards legally enforceable hate speech detection for public forums\"), mdx(\"p\", null, mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://github.com/chufeiluo/legalhatespeech\"\n  }, \"Github\")), mdx(\"h2\", {\n    \"id\": \"project-description\"\n  }, \"Project Description\"), mdx(\"p\", null, \"Hate speech causes widespread and deep-seated societal issues. Proper enforcement of hate speech laws is key for protecting groups of people against harmful and discriminatory language. However, determining what constitutes hate speech is a complex task that is highly open to subjective interpretations. Existing works do not align their systems with enforceable definitions of hate speech, which can make their outputs inconsistent with the goals of regulators. \"), mdx(\"p\", null, \"This research introduces a new perspective and task for enforceable hate speech detection centred around legal definitions, and a dataset annotated on violations of eleven possible definitions by legal experts. Given the challenge of identifying clear, legally enforceable instances of hate speech, we augment the dataset with expert-generated samples and an automatically mined challenge set. We experiment with grounding the model decision in these definitions using zero-shot and few-shot prompting. We then report results on several large language models (LLMs). \"), mdx(\"p\", null, \"With this task definition, automatic hate speech detection can be more closely aligned to enforceable laws, and hence assist in more rigorous enforcement of legal protections against harmful speech in public forums.\"), mdx(\"h2\", {\n    \"id\": \"significance\"\n  }, \"Significance\"), mdx(\"p\", null, \"This project studied:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"How to ground evaluations to be meaningful to another, specialized audience\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"It was one of the first to benchmark LLMs - most of the experiments were completed January 2023 - against existing prompt tuning methods\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"What is the source of ambiguity in other hate speech datasets\")), mdx(\"p\", null, \"The paper also studied how to use self-training with silver labels to improve performance to match GPT-4 but with a much smaller model (RoBERTa-Large). This was only on a 3-way classification task and, as found in follow-up studies (\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/sae-pd\",\n    \"title\": \"sae-pd\"\n  }, \"[[sae-pd]]\"), \"), there are some lexical features that allow LLMs to learn this classification label really well with few-shot prompting. \"), mdx(\"h2\", {\n    \"id\": \"post-mortem\"\n  }, \"Post mortem\"), mdx(\"p\", null, \"I still like this project. I think I'm still proud of it. I don't think the reasoning analysis is in-depth enough by today's standards, and my thoughts on model training vs. prompting flip-flop every so often, but it's been a while since I've had a paper compare model training to prompting. \"));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"__typename":"File","id":"af4ce0a7-005b-5368-8770-5764a771d61e","fields":{"slug":"/completed/legalhatespeech","title":"Towards legally enforceable hate speech detection for public forums"}}},{"__typename":"Mdx","body":"var _excluded = [\"components\"];\nfunction _extends() { return _extends = Object.assign ? Object.assign.bind() : function (n) { for (var e = 1; e < arguments.length; e++) { var t = arguments[e]; for (var r in t) ({}).hasOwnProperty.call(t, r) && (n[r] = t[r]); } return n; }, _extends.apply(null, arguments); }\nfunction _objectWithoutProperties(e, t) { if (null == e) return {}; var o, r, i = _objectWithoutPropertiesLoose(e, t); if (Object.getOwnPropertySymbols) { var n = Object.getOwnPropertySymbols(e); for (r = 0; r < n.length; r++) o = n[r], -1 === t.indexOf(o) && {}.propertyIsEnumerable.call(e, o) && (i[o] = e[o]); } return i; }\nfunction _objectWithoutPropertiesLoose(r, e) { if (null == r) return {}; var t = {}; for (var n in r) if ({}.hasOwnProperty.call(r, n)) { if (-1 !== e.indexOf(n)) continue; t[n] = r[n]; } return t; }\n/* @jsxRuntime classic */\n/* @jsx mdx */\n\nvar _frontmatter = {\n  \"title\": \"Completed Projects\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n    props = _objectWithoutProperties(_ref, _excluded);\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"completed-projects\"\n  }, \"Completed Projects\"), mdx(\"p\", null, \"Here you can find the repositories and papers for projects I would consider complete-ish. Some pages have post-mortem reflections - what I could have done better, what is left to be explored, etc.\"), mdx(\"p\", null, \"These are all projects where I was the first author; this means I wrote the entire codebase (except for BiasKG, where I designed the core knowledge graph algorithm and my co-author Ahmed implemented it, and then we ran half of the experiments each) and paper.\"), mdx(\"h2\", {\n    \"id\": \"interpretability\"\n  }, \"Interpretability\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/prototypes\",\n    \"title\": \"prototypes\"\n  }, \"[[prototypes]]\"), \" - Making citation prediction more easily understood by lawyers with prototype theory, precedents, and legislation\"), mdx(\"h2\", {\n    \"id\": \"datasets\"\n  }, \"Datasets\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/legalhatespeech\",\n    \"title\": \"legalhatespeech\"\n  }, \"[[legalhatespeech]]\"), \" - Grounding the subjectivity of hate speech in 11 definitions from various legal authorities\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/misinformation\",\n    \"title\": \"misinformation\"\n  }, \"[[misinformation]]\"), \" - Grounding the assessment of misinformation in possible legal implications\"), mdx(\"h2\", {\n    \"id\": \"bias\"\n  }, \"Bias\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/biaskg\",\n    \"title\": \"biaskg\"\n  }, \"[[biaskg]]\"), \" - Building a dynamic knowledge graph from natural language for assessing social bias in LLMs\"), mdx(\"h2\", {\n    \"id\": \"test-time-alignment\"\n  }, \"Test-time Alignment\"), mdx(\"p\", null, \"\", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/sae-pd\",\n    \"title\": \"sae-pd\"\n  }, \"[[sae-pd]]\"), \" - Investigating model steering with Sparse Autoencoders (SAEs) for pluralistic alignment on sparse legal data\"));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"__typename":"File","id":"43c4e06e-a3ed-5032-881b-07cea9e40167","fields":{"slug":"/indexes/completed-projects","title":"Completed Projects"}}}]},"fields":{"slug":"/completed/sae-pd","title":"Towards Low-Resource Alignment to Diverse Perspectives with Sparse Feedback"}}},"pageContext":{"id":"5a4d9fbb-e68a-5c47-bdb2-4b374aa740dd"}},"staticQueryHashes":["2098632890","2221750479","2468095761"]}